{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Check Questions</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 1**: Чем отличается AdaBoost от XGBoost? Перечислите принципиальные отличия. \n",
    "\n",
    "1) Решают разные задачи: AdaBoost хорошо решает задачу классификации для двух классов и плохо адаптируется для других задач (регрессии и многоклассовой классификации). В то же время XGBoost подходит и для регрессии, и для многоклассовой классификации, являясь одним из самых мощных и универсальных алгоритмов на сегодняшний день.\n",
    "\n",
    "2) AdaBoost - метод, который предполагает выбор семейства базовых классификаторов, обладающее свойством *слабой обучаемости*. XGBoost - это реализация градиентного бустинга, использующая в качестве семейства базовых алгоритмов решающие деревья.\n",
    "\n",
    "3) Для выбора алгоритма следующего шага используются разные функционалы качества.<br>\n",
    "В AdaBoost используется экспоненциальный: $Q(a, X_l) = \\sum_{i = 1}^l{(exp(−y_ia(x_i))} → min_a$. <br>\n",
    "В XGBoost же используется регуляризованный функционал $(H(b))$, то есть помимо измерения ошибки композиции алгортимов в него также добавлены штрафы за количество листев и за норму коэффициентов. $H(b) = \\dfrac{1}{2}\\sum_{j=1}^{J}{\\dfrac{S_j^2}{H_j+\\mu}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 2**: Почему говорят, что AdaBoost неустойчив к выбросам?\n",
    "\n",
    "Экспоненциальная функция потерь, используемая в AdaBoost, крайне чувствительна к выбросам, так как наличие объекта с большим отрицательным отступом приведет к тому, что этот объект получит большой вес, и обучение будет слишком сильно концентрироваться на нем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 3**:  В каком пространстве градиентный бустинг совершает градиентный спуск? Какова размерность этого пространства?\n",
    "\n",
    "Градиентный бустинг осуществляет градиентный спуск в пространстве прогнозов алгоритма на обучающей выборке. Размерность этого пространства равна размеру обучающей выборки, то есть $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 4**: В чем заключается сокращение шага в градиентном бустинге? Как число итераций, необходимое для сходимости, зависит от размера шага η?\n",
    "\n",
    "На практике градиентный бустинг очень быстро строит композицию, после чего начинает настраиваться на шум и переобучаться. Действенным решением данной проблемы является *сокращение шага*: вместо перехода в оптимальную точку в направлении антиградиента делается укороченный шаг\n",
    "$a_N(x) = a_{N-1}(x)+\\eta\\gamma_Nb_N(x)$, где $\\eta\\in(0,1]$ - темп обучения.\n",
    "Число итераций, необходимое для сходимости, возрастает с уменьшением шага $\\eta$, поскольку чем меньше шаг, тем меньше за одну итерацию алгоритм приближается к точке сходимости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 5**: Что такое стохастический градиентный бустинг?\n",
    "\n",
    "Это градиентный бустинг, в котором для улучшения качества используется внесение рандомизации в процесс обучения базовых алгоритмов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">Boosting</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2) Найдите градиент логистичиской функции потерь для фиксированного объекта\n",
    "\n",
    "$\\left.\\dfrac{\\partial{L}}{\\partial{z}}\\right|_{z = a_{N-1}(x_i)} = \\dfrac{y_i}{1 + exp(y_ia_{N-1}(x_i))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Binary Boosting Implementation</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно реализовать двухклассовый бустинг с логистичиской функцией потерь. \n",
    "\n",
    "Длину шага -- или используйте $1.0*lr$ или подбирайте одномерной оптимизацией;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from utils import plot_surface\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BinaryBoostingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_estimators, lr=0.1):\n",
    "        self.lr = lr\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def loss_grad(self, original_y, pred_y):\n",
    "        return # Градиент на кажом объекте\n",
    "\n",
    "    def fit(self, X, original_y):\n",
    "        \n",
    "        self.estimators_ = []\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            grad = self.loss_grad(original_y, self._predict(X))\n",
    "            # Настройте базовый алгоритм на градиент, это классификация или регрессия?\n",
    "            estimator = \n",
    "            self.estimators_.append(estimator)\n",
    "        \n",
    "        self.out_ = self.outliers(grad)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _predict(self, X):\n",
    "        y_pred = <Получите ответ композиции до применения решающего правила>\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = <примените к self._predict решающее правило>\n",
    "        return y_pred\n",
    "    \n",
    "    def outliers(self, grad):\n",
    "        return # Топ-10 объектов с большим отступом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Simple test</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=2,\n",
    "                           n_informative=2, n_redundant=0, n_repeated=0,\n",
    "                           n_classes=2, n_clusters_per_class=2,\n",
    "                           flip_y=0.05, class_sep=0.8, random_state=241)\n",
    "y = 2*(y-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = BinaryBoostingClassifier(n_estimators=100).fit(X, y)\n",
    "plot_surface(X, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">Outliers</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<Нарисуйте только outliers> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Adult test</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sh ./get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adult = pd.read_csv(\n",
    "    './data/adult.data', \n",
    "    names=[\n",
    "        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "        \"Hours per week\", \"Country\", \"Target\"], \n",
    "    header=None, na_values=\"?\")\n",
    "adult = pd.get_dummies(adult)\n",
    "adult[\"Target\"] = adult[\"Target_ >50K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = adult[adult.columns[:-3]].values, adult[adult.columns[-1]].values\n",
    "y = 2*(y-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<Сверьте качество своего алгоритма с GradientBoostingClassifier>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Text classification</h1> \n",
    "\n",
    "- Найдите двухклассовый текстовый датасет (в качестве примера sentiment analysis) или возьмите многоклассовый и классифцируйте один клас против остальных\n",
    "- Попробуйте бустинг на решающих деревьях, в качестве фичей используйте tf-idf и svd/random_projection/hashing_trick, что работает лучше? Сравните качество и время работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Random Forest vs Boosting</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Подберите 2+ датасета как минимум большой и маленький (не использованных в этом дз) и проведите сравнение random forest и градиентного бустинга, используйте реализации алгоритмов из библиотеки sklearn.\n",
    "\n",
    "- Опишите результаты, почему тот или другой алгоритм на конкретном датасете работает лучше/хуже?\n",
    "- Как вы настраивали гиперпараметры алгоритмов?\n",
    "- Как вы проверяли качесво алгоритмов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">Bonus part</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это удвоит баллы за дз.\n",
    "\n",
    "- Реализуйте мультиклассовый бустинг -- проверьте на CIFAR10 + SVD\n",
    "- Попробуйте различные функции потерь, придумайте несколько своих, удалось ли обойти логистичискую и экспоненциальную?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
